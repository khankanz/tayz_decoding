{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tayz_decoding.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tayz_decoding\n",
    "\n",
    "> Fast decoding using CRANE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to using `nbdev` here are some useful pointers to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install tayz_decoding in Development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "# make sure tayz_decoding package is installed in development mode\n",
    "$ pip install -e .\n",
    "\n",
    "# make changes under nbs/ directory\n",
    "# ...\n",
    "\n",
    "# compile to have changes apply to tayz_decoding\n",
    "$ nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install latest from the GitHub [repository][repo]:\n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/khankanz/tayz_decoding.git\n",
    "```\n",
    "\n",
    "[repo]: https://github.com/khankanz/tayz_decoding\n",
    "[docs]: https://khankanz.github.io/tayz_decoding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Conda env ('crane')\n",
    "This environment is configured for CUDA 12.1 + PyTorch with CUDA support, plus `xgrammar`, `transformers` and a CUDA-accelerated build of `llama-cpp-python`\n",
    "##### 1. Create and activate the env\n",
    "```bash\n",
    "conda create -n crane python=3.10 -y\n",
    "conda activate crane\n",
    "```\n",
    "##### 2. Install NVIDIA CUDA Toolkit via conda (recommended)\n",
    "This pulls the official NVIDIA libraries that match the driver on your machine.\n",
    "```bash\n",
    "conda install -c nvidia cuda-toolkit=12.1 -y\n",
    "```\n",
    "**Important note about CUDA compatibility**\n",
    "* NVIDIA drivers are forward-compatible: a driver that supports CUDA 12.1 (or newer) can run applications built against CUDA 12.1, 12.2, 12.3 etc. \n",
    "* Run `nvidia-smi` - the 'CUDA Version' column in the top-right shows the maximum CUDA runtime your driver supports. As long as that number is >= 12.1, this env will get full GPU acceleration. \n",
    "##### 3. Install PyTorch with CUDA 12.1 wheels\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "##### 4. Install `xgrammar` without its dependencies\n",
    "`xgrammar` currently pulls in dependencies that can conflict with versions we need later. Install it with `--no-deps` first. We manually install the exact versions we want right after:\n",
    "```bash\n",
    "pip install xgrammar --no-deps\n",
    "```\n",
    "##### 5. Install core dependencies\n",
    "Always use `--dry-run` first! This lets you see exactly which versions/wheels will be installed or upgraded *before* anything happens. It prevents accidental CUDA mismatches or huge re-downloads.\n",
    "```bash\n",
    "pip install pydantic transformers ninja --dry-run\n",
    "```\n",
    "* if the dry-runs look good, run them for real by removing flag\n",
    "##### 6. Install CUDA-accelerated `llama-cpp-python`\n",
    "This step compiles `llama-cpp-python` with GPU support (GMML -> CUDA)\n",
    "```bash\n",
    "# First: dry-run to verify it will compile and not try to pull wrong CUDA wheels\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --dry-run\n",
    "\n",
    "# If everything looks correct â†’ install for real\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --verbose\n",
    "```\n",
    "* The `--verbose` flag is helpful the first time so you can see the cmake/ninja output and confirm it's actually detecting and using your CUDA toolkit.\n",
    "##### 7. Final\n",
    "```bash\n",
    "python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"\n",
    "python -c \"import llama_cpp; print('llama-cpp-python built with CUDA:', llama_cpp.__cuda__)\"\n",
    "pip list | grep -E \"(torch|xgrammar|transformers|llama-cpp-python)\"\n",
    "```\n",
    "\n",
    "Congratulations, you should now have a fully working `crane` env with GPU-accelerated PyTorch, HuggingFace transformers, `xgrammar` and `llama-cpp-python`. Don't forget to pip install this lib now; \n",
    "```bash\n",
    "pip install git+https://github.com/khankanz/tayz_decoding.git\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
