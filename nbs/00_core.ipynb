{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zardar/miniconda3/envs/crane/lib/python3.10/site-packages/nbdev/doclinks.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources,importlib\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at what the regular llama-cpp flow looks like, as per their docs. I'm going to use `Tiny Llama 1.1B` as my test model. You will need to download a more capable model and setup the path pointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zardar/Downloads/tinyllama-1.1b-chat-v1.0.Q2_K.gguf'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from pathlib import Path\n",
    "tl1B_path=str(Path(\"/home/zardar/Downloads/tinyllama-1.1b-chat-v1.0.Q2_K.gguf\")) ; tl1B_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /home/zardar/Downloads/tinyllama-1.1b-chat-v1.0.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q2_K:   45 tensors\n",
      "llama_model_loader: - type q3_K:  110 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 459.11 MiB (3.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 22\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 5632\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.10 B\n",
      "print_info: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 200 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   459.11 MiB\n",
      "......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    11.00 MiB\n",
      "llama_kv_cache_unified: size =   11.00 MiB (   512 cells,  22 layers,  1/1 seqs), K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 1608\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =    66.50 MiB\n",
      "llama_context: graph nodes  = 776\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=tl1B_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     582.87 ms\n",
      "llama_perf_context_print: prompt eval time =     582.53 ms /    47 tokens (   12.39 ms per token,    80.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     131.71 ms /     5 runs   (   26.34 ms per token,    37.96 tokens per second)\n",
      "llama_perf_context_print:       total time =     716.74 ms /    52 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-15dbd0fc-6fe7-496f-a09b-c9e8110a8f5a',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1763237767,\n",
       " 'model': '/home/zardar/Downloads/tinyllama-1.1b-chat-v1.0.Q2_K.gguf',\n",
       " 'choices': [{'text': 'Microscopic examination reveals nests of atypical squamous cells with keratinization, diagnostic of squamous cell carcinoma. Q: What is the Histologic type? A: Keratinization. ',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 47, 'completion_tokens': 6, 'total_tokens': 53}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Microscopic examination reveals nests of atypical squamous cells with keratinization, diagnostic of squamous cell carcinoma. Q: What is the Histologic type? A:\", \n",
    "    max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different formats, we can use `chat_completion` approach. There's also `JSON` and `JSON Schema Mode`. These use grammars to enforce structure but it's a bit simplistic for our purposes. Now the same task as above expressed as `chat_completions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] **TODO:** expand on kwargs such as `n_ctx`, especially `n_gpu_layers=-1`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     582.87 ms\n",
      "llama_perf_context_print: prompt eval time =     899.08 ms /    82 tokens (   10.96 ms per token,    91.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     697.40 ms /    27 runs   (   25.83 ms per token,    38.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1607.69 ms /   109 tokens\n",
      "llama_perf_context_print:    graphs reused =         25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-3aff67c4-9f2c-405a-9340-9ccd20ec1f90',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1763237768,\n",
       " 'model': '/home/zardar/Downloads/tinyllama-1.1b-chat-v1.0.Q2_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'The Histologic type in the following report is \"Squamo-Squamo-Keratocarcinoma.\"'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 82, 'completion_tokens': 27, 'total_tokens': 109}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a medical assistant specialized in cancer reporting.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What is the Histologic type in the following report: Microscopic examination reveals nests of atypical squamous cells with keratinization, diagnostic of squamous cell carcinoma.\"\n",
    "          }\n",
    "      ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Ignore the incorrect terminology, this is purely for demonstrative purposes. Normally, we'd pick a much larger and more suitable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zardar/miniconda3/envs/crane/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "#| export\n",
    "from fastcore.basics import patch\n",
    "from tayz_decoding.types import CreateCRANEChatCompletionResponse\n",
    "from typing import List, Dict, Type, Generator, Tuple\n",
    "from pydantic import BaseModel\n",
    "import xgrammar as xgr\n",
    "from xgrammar import TokenizerInfo\n",
    "import numpy as np\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrating `xgrammar` with `llama-cpp-python`'s Low-Level Sampler**\n",
    "* The standard way to guide text generation in libraries like HuggingFace's transformers is with a `LogitsProcessor`. This is a high-level workflow that receives a full array of logits (probabilities for every token in the vocabulary) at each step and modifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT**\n",
    "* `llama-cpp-python` is highly optimized for performance and avoids copying full logits tensor from its C++ core to Python at every token, this would be computational costly. Instead, it provides a more efficient, low-level callback mechnism: the `LlamaSampler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaSampler**\n",
    "1. *Candidate Selection*: The C++ core first runs buil-in samplers (like top-k or top-p), this reduces the # of possible next tokens to a small candidate set.\n",
    "2. *Callback invocation*: It then invokes a cb function, `apply_func`, and passes it a C-level pointer to this small candidate set.\n",
    "3. Our logic modifies the logits of only these few candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "* a custom sampler, `_make_xgr_sampler` that bridges `xgrammar` and `LlamaSampler`\n",
    "* `xgrammar` expects a full logit vector, entire vocabulary\n",
    "* `LlamaSampler` only provides a partial one\n",
    "* *Scatter-Apply-Gather* pattern; we manually reconstruct the full logit tensor in Python, let `xgrammar` apply its full-vocabulary mask and gather the modified logits from the original candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _crane_build_xgr_compiler(self: Llama) -> xgr.GrammarCompiler:\n",
    "    \"\"\"\n",
    "    Builds and caches an xgrammar.GrammarCompiler for the current Llama model.\n",
    "    Essential for converting JSON schemas into a format the grammar matcher can use.\n",
    "    \"\"\"\n",
    "    if hasattr(self, \"_xgr_compiler\"): return self._xgr_compiler\n",
    "    #raw_toks = [self._model.token_get_text(i) for i in range(vocab_size)]\n",
    "    raw_toks = [self.detokenize([i], special=True) for i in range(self.n_vocab())]\n",
    "    tok_info = xgr.TokenizerInfo(encoded_vocab=raw_toks, vocab_type=xgr.VocabType.RAW,\n",
    "                                 vocab_size=self.n_vocab(), stop_token_ids=[self.token_eos()],\n",
    "                                 add_prefix_space=True)\n",
    "    self._xgr_compiler = xgr.GrammarCompiler(tok_info)\n",
    "    return self._xgr_compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _make_xgr_matcher(self: Llama, schema: Type[BaseModel]) -> tuple[xgr.GrammarMatcher, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Creates an xgrammar.GrammarMatcher and a token bitmask for a given Pydantic schema.\n",
    "    The matcher tracks the generation state against the schema, and the bitmask is the low-level data \n",
    "    structure used to enable/disable tokens.\n",
    "    \"\"\"\n",
    "    compiler = self._crane_build_xgr_compiler()\n",
    "    cg = compiler.compile_json_schema(json.dumps(schema.model_json_schema()), any_whitespace=True, strict_mode=True)\n",
    "    matcher = xgr.GrammarMatcher(cg)\n",
    "    bitmask = xgr.allocate_token_bitmask(batch_size=1, vocab_size=self.n_vocab())\n",
    "    return matcher, bitmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_cpp._internals import LlamaSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# In llama-cpp-xgrammar-crane.py\n",
    "@patch\n",
    "def _make_xgr_sampler(self: Llama, matcher: xgr.GrammarMatcher, bitmask: torch.Tensor) -> LlamaSampler:\n",
    "    \"\"\"\n",
    "    Creates a custom LlamaSampler that integrates xgrammar constraints into the llama.cpp sampling pipeline.\n",
    "    \"\"\"\n",
    "    from llama_cpp._internals import LlamaSampler\n",
    "\n",
    "    # Determine the device to use for tensor operations\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def apply_func(cur_p):\n",
    "        # 1) Get the candidate logits and their token IDs from llama.cpp's sampler.\n",
    "        #    This is a PARTIAL list, not the full vocabulary.\n",
    "        sz = cur_p.contents.size\n",
    "        arr = np.ctypeslib.as_array(cur_p.contents.data, shape=(sz,))\n",
    "        logits_np = arr[\"logit\"]  # The logits for the candidates\n",
    "        ids_np = arr[\"id\"].astype(np.int32) # The token IDs for the candidates\n",
    "        ids_t = torch.from_numpy(ids_np).to(device)\n",
    "\n",
    "        # 2) If grammar has terminated, force EOS and exit early.\n",
    "        if matcher.is_terminated():\n",
    "            logits_np[...] = -np.inf\n",
    "            eos_token_id = self.token_eos()\n",
    "            # Find if EOS is in our candidate set and set its logit to 0.\n",
    "            eos_rows = np.where(ids_np == eos_token_id)[0]\n",
    "            if eos_rows.size > 0:\n",
    "                logits_np[eos_rows[0]] = 0.0\n",
    "            return\n",
    "\n",
    "        # 3) Create a full-sized logits tensor initialized to negative infinity.\n",
    "        #    This is the \"Scatter\" step.\n",
    "        vocab_size = self.n_vocab()\n",
    "        full_logits_t = torch.full((vocab_size,), -float('inf'), dtype=torch.float32, device=device)\n",
    "        \n",
    "        # 4) Place the candidate logits into the full tensor at their correct positions.\n",
    "        full_logits_t[ids_t] = torch.from_numpy(logits_np).to(device)\n",
    "\n",
    "        # 5) Compute the next-token bitmask from the grammar matcher.\n",
    "        xgr.reset_token_bitmask(bitmask)\n",
    "        matcher.fill_next_token_bitmask(bitmask, index=0)\n",
    "\n",
    "        # 6) Apply the full-vocabulary bitmask to the full-vocabulary logits tensor.\n",
    "        #    The bitmask is on the CPU, so we move it to the correct device.\n",
    "        xgr.apply_token_bitmask_inplace(full_logits_t, bitmask.to(device), vocab_size=vocab_size)\n",
    "\n",
    "        # 7) Gather the modified logits for the original candidates back into the numpy array\n",
    "        #    that llama.cpp will read from. This is the \"Gather\" step.\n",
    "        modified_logits = full_logits_t[ids_t].cpu().numpy()\n",
    "        np.copyto(logits_np, modified_logits)\n",
    "        \n",
    "        # 8) Backstop: If all candidate logits became -inf (due to a grammar mismatch),\n",
    "        #    force the EOS token to prevent the sampler from failing.\n",
    "        if not np.isfinite(logits_np).any():\n",
    "            logits_np[...] = -np.inf\n",
    "            eos_token_id = self.token_eos()\n",
    "            eos_rows = np.where(ids_np == eos_token_id)[0]\n",
    "            if eos_rows.size > 0:\n",
    "                logits_np[eos_rows[0]] = 0.0\n",
    "            else:\n",
    "                # If EOS wasn't even a candidate, just pick the first candidate to avoid a total crash.\n",
    "                if len(logits_np) > 0:\n",
    "                    logits_np[0] = 0.0\n",
    "\n",
    "    sampler = LlamaSampler()\n",
    "    sampler.add_custom(apply_func)\n",
    "    sampler.add_greedy()\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_ctx(self: Llama):\n",
    "    \"\"\"\n",
    "    Internal helper to safely access the underlying llama.cpp context\n",
    "    \"\"\"\n",
    "    ctx = getattr(self, \"_ctx\", None)\n",
    "    if ctx is None or not hasattr(ctx, \"ctx\"): raise TypeError(\"Expected a LlamaContext as self._ctx (with .ctx handle)\")\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _find_subseq(a: List[int], sub: List[int]) -> int:\n",
    "    \"\"\"\n",
    "    Helper function to find a sub-sequence of tokens.\n",
    "    \"\"\"\n",
    "    if not sub: return -1\n",
    "    L,M = len(a), len(sub)\n",
    "    for i in range(max(0,L-M),L):\n",
    "        if a[i:i+M] == sub: return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _crane_generate_unconstrained(self: Llama, s1: str, max_toks: int, temperature: float, stop: List[str]) -> Generator[str, None, Tuple[List[int], str]]:\n",
    "    \"\"\"\n",
    "    Generates text tokens by token in unconstrained mode.\n",
    "    Yields: Detokenized text for each token\n",
    "    Returns: A tuple of (all_generated_tokens, stop_reason)\n",
    "    \"\"\"\n",
    "    s1_toks = self.tokenize(s1.encode(\"utf-8\"), add_bos=False, special=True)\n",
    "    stop_seq_toks = [self.tokenize(s.encode(\"utf-8\"), add_bos=False, special=True) for s in stop if s]\n",
    "    \n",
    "    gen_toks: List[int] = []\n",
    "    gen_text: str = \"\"\n",
    "\n",
    "    for _ in range(max_toks): \n",
    "        tok = self.sample(temp=temperature)\n",
    "        if tok == self.token_eos(): return gen_toks, \"eos\"\n",
    "\n",
    "        gen_toks.append(tok)\n",
    "        chunk = self.detokenize([tok], special=False).decode(\"utf-8\", errors=\"ignore\")\n",
    "        gen_text += chunk; yield chunk\n",
    "\n",
    "        for stop_seq in stop_seq_toks:\n",
    "            seq_len = len(stop_seq)\n",
    "            if len(gen_toks) >= seq_len and gen_toks[-seq_len:] == stop_seq: return gen_toks[:-seq_len], \"stop\"\n",
    "        if len(gen_toks) >= len(s1_toks) and gen_toks[-len(s1_toks):] == s1_toks: return gen_toks, \"s1\"\n",
    "        self.eval([tok])\n",
    "    return gen_toks, \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _crane_generate_constrained(self: Llama, schema: Type[BaseModel], s2_toks: List[int], max_toks:int, prefix_toks_after_s1: List[int] | None = None) -> Generator[str, None, List[int]]:\n",
    "    \"\"\"\n",
    "        Generates text token by token in constrained mode.\n",
    "        Yields: Detokenized text for each token.\n",
    "        Returns: The list of tokens generated in this phase\n",
    "    \"\"\"\n",
    "    matcher, bitmask = self._make_xgr_matcher(schema)\n",
    "    \n",
    "    sampler = self._make_xgr_sampler(matcher, bitmask)\n",
    "    ctx = self._get_ctx()\n",
    "\n",
    "    if prefix_toks_after_s1:\n",
    "        for t in prefix_toks_after_s1: matcher.accept_token(t)\n",
    "    \n",
    "    generated_toks_in_phase: List[int] = []\n",
    "\n",
    "    try:\n",
    "        for _ in range(max_toks):\n",
    "            tok = sampler.sample(ctx)\n",
    "            if tok == self.token_eos() or matcher.is_terminated(): break\n",
    "            generated_toks_in_phase.append(tok)\n",
    "\n",
    "            sampler.accept(tok)\n",
    "            matcher.accept_token(tok) # advance grammar for next step\n",
    "            self.eval([tok]) # advance model KV    \n",
    "\n",
    "            yield self.detokenize([tok], special=False).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            if len(generated_toks_in_phase) >= len(s2_toks) and generated_toks_in_phase[-len(s2_toks):] == s2_toks: break\n",
    "    finally:\n",
    "        matcher.reset()\n",
    "        sampler.reset()\n",
    "\n",
    "    return generated_toks_in_phase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from llama_cpp import LogitsProcessorList\n",
    "from llama_cpp.llama_types import ChatCompletionResponseChoice, ChatCompletionResponseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_cpp.llama_chat_format import Jinja2ChatFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mJinja2ChatFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meos_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbos_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_token_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[int]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base Protocol for a chat formatter. A chat formatter is a function that\n",
      "takes a list of messages and returns a chat format response which can be used\n",
      "to generate a completion. The response can also include a stop token or list\n",
      "of stop tokens to use for the completion.\n",
      "\u001b[0;31mInit docstring:\u001b[0m A chat formatter that uses jinja2 templates to format the prompt.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/crane/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py\n",
      "\u001b[0;31mType:\u001b[0m           _ProtocolMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?Jinja2ChatFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def create_crane_chat_completion(self: Llama, messages: List[Dict[str,str]], schema=Type[BaseModel],\n",
    "                                 s1:str = \"<<JSON>>\", s2:str=\"</JSON>>\", temperature:float=0.0,\n",
    "                                 max_tokens_unconstrained:int=3072,max_tokens_constrained:int=4096, \n",
    "                                 stop: List[str] = []) -> CreateCRANEChatCompletionResponse: \n",
    "        \"\"\"\n",
    "        CRANE: Constrained + Unconstrained switching for structured output.\n",
    "        \n",
    "        Generates text until `s1` delimiter (unconstrained), then switches to\n",
    "        constrained JSON generation matching `pydantic_model` schema until `s2`.\n",
    "        \"\"\"\n",
    "        # reset model state\n",
    "        self.reset()\n",
    "        etok = self._model.token_get_text(self.token_eos())\n",
    "        btok = self._model.token_get_text(self.token_bos())\n",
    "        formatter = Jinja2ChatFormatter(template=self.metadata['tokenizer.chat_template'],\n",
    "                                        eos_token=etok, bos_token=btok)\n",
    "\n",
    "        resp = formatter(messages=messages)\n",
    "        f_prmpt: str = resp.prompt\n",
    "        \n",
    "        prmpt_toks = self.tokenize(f_prmpt.encode(\"utf-8\"), add_bos=True)\n",
    "        self.eval(prmpt_toks) # evaluate initial prompt\n",
    "        \n",
    "        # tokenize delimiters and stops\n",
    "        s1_toks = self.tokenize(s1.encode(\"utf-8\"), add_bos=False, special=True)\n",
    "        s2_toks = self.tokenize(s2.encode(\"utf-8\"), add_bos=False, special=True)\n",
    "        \n",
    "        # Phase 1. Unconstrained generation until s1 or max_tokens\n",
    "        unc_gen = self._crane_generate_unconstrained(\n",
    "                s1=s1, max_toks=max_tokens_unconstrained, temperature=temperature,\n",
    "                stop=stop)\n",
    "        unc_text, unc_toks, stop_reason = \"\", [], \"error\"\n",
    "        while True:\n",
    "                try: chunk = next(unc_gen)\n",
    "                except StopIteration as e:\n",
    "                        if e.value is not None: unc_toks, stop_reason = e.value\n",
    "                        break\n",
    "                unc_text += chunk\n",
    "        \n",
    "        if stop_reason != 's1':\n",
    "                if s1 in unc_text: \n",
    "                        self.eval(s1_toks)\n",
    "                        stop_reason = \"s1\"\n",
    "                elif stop_reason == \"eos\":\n",
    "                        self.eval(s1_toks)\n",
    "                        unc_text += s1\n",
    "                        stop_reason = \"s1\"\n",
    "                else:\n",
    "                        raise AssertionError(f\"s1 delimiter '{s1}' not found in unconstrained phase.\\n{unc_text}\\n{stop_reason}\")\n",
    "\n",
    "        s1_pos = _find_subseq(unc_toks, s1_toks)\n",
    "        if s1_pos == -1: prefix_toks_after_s1 = s1_toks\n",
    "        else: prefix_toks_after_s1 = unc_toks[s1_pos:]\n",
    "        self.eval(s1_toks)\n",
    "        \n",
    "        # Phase 2. Constrained generation\n",
    "        con_gen = self._crane_generate_constrained(schema=schema, s2_toks=s2_toks, \n",
    "                                                              max_toks=max_tokens_constrained, prefix_toks_after_s1=prefix_toks_after_s1)\n",
    "        jtxt, con_toks = \"\", []\n",
    "        while True:\n",
    "                try: chunk = next(con_gen)\n",
    "                except StopIteration as e:\n",
    "                        if e.value is not None: con_toks = e.value\n",
    "                        break\n",
    "                jtxt +=chunk\n",
    "                \n",
    "        unc_text += jtxt\n",
    "        unc_text += s2\n",
    "        #assert unc_text.endswith(s2), f\"s2 delimiter '{s2}' not found at end of constrained phase.\\nJTXT: {jtxt}\"\n",
    "\n",
    "        try:\n",
    "                pjson = schema.model_validate_json(jtxt)\n",
    "        except Exception as e: raise ValueError(f\"Failed to parse generated JSON: {jtxt}\\nError:{e}\")\n",
    "        \n",
    "        cc = ChatCompletionResponseChoice(index=0,\n",
    "                                message=ChatCompletionResponseMessage(role=\"assistant\", content=jtxt), finish_reason=\"stop\")\n",
    "        \n",
    "        return CreateCRANEChatCompletionResponse(id=\"crane-\"+str(id(self)), object=\"crane.chat.completion\",\n",
    "                                                 completion=cc, json=pjson) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KV cache in a decoder-only transformer is analogous to the encoder output in an encoder–decoder model — it’s the stored representation of the prefix context that subsequent tokens attend to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out on our similar use case from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class HistologicType(BaseModel):\n",
    "    histologic_type: Literal[\"Keritinzation\", \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'histologic_type': {'enum': ['Keritinzation', 'None'],\n",
       "   'title': 'Histologic Type',\n",
       "   'type': 'string'}},\n",
       " 'required': ['histologic_type'],\n",
       " 'title': 'HistologicType',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HistologicType.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't include `s1` and `s2` in the user messages anymore because they're control delimiters, not part of the natural language exchange. The orchestrator appends `s1` at runtime right before switching to grammar-constraned decoding, and later expects `s2` to mark the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pathology assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify the histologic type from the following report: Microscopic examination reveals nests of atypical squamous cells with keratinization, diagnostic of squamous cell carcinoma.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if a given model has a defined chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.metadata['tokenizer.chat_template']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._model.token_get_text(llm.token_bos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.create_crane_chat_completion(\n",
    "    messages=messages,\n",
    "    schema=HistologicType,\n",
    "    s1=\"<<JSON>>\",\n",
    "    s2=\"</JSON>>\",\n",
    "    temperature=0.0,\n",
    "    max_tokens_unconstrained=512,\n",
    "    max_tokens_constrained=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'crane-138206126811712',\n",
       " 'object': 'crane.chat.completion',\n",
       " 'completion': {'index': 0,\n",
       "  'message': {'role': 'assistant', 'content': '{\"histologic_type\": \"None\"}'},\n",
       "  'finish_reason': 'stop'},\n",
       " 'json': HistologicType(histologic_type='None')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
