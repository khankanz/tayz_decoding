"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/00_core.ipynb 3
from llama_cpp import Llama

# %% ../nbs/00_core.ipynb 12
from fastcore.basics import patch
from .types import CreateCRANEChatCompletionResponse
from typing import List, Dict, Type, Generator, Tuple
from pydantic import BaseModel
import xgrammar as xgr
from xgrammar import TokenizerInfo
import numpy as np
import json
import torch

# %% ../nbs/00_core.ipynb 17
@patch
def _crane_build_xgr_compiler(self: Llama) -> xgr.GrammarCompiler:
    """
    Builds and caches an xgrammar.GrammarCompiler for the current Llama model.
    Essential for converting JSON schemas into a format the grammar matcher can use.
    """
    if hasattr(self, "_xgr_compiler"): return self._xgr_compiler
    #raw_toks = [self._model.token_get_text(i) for i in range(vocab_size)]
    raw_toks = [self.detokenize([i], special=True) for i in range(self.n_vocab())]
    tok_info = xgr.TokenizerInfo(encoded_vocab=raw_toks, vocab_type=xgr.VocabType.RAW,
                                 vocab_size=self.n_vocab(), stop_token_ids=[self.token_eos()],
                                 add_prefix_space=True)
    self._xgr_compiler = xgr.GrammarCompiler(tok_info)
    return self._xgr_compiler

# %% ../nbs/00_core.ipynb 18
@patch
def _make_xgr_matcher(self: Llama, schema: Type[BaseModel]) -> tuple[xgr.GrammarMatcher, torch.Tensor]:
    """
    Creates an xgrammar.GrammarMatcher and a token bitmask for a given Pydantic schema.
    The matcher tracks the generation state against the schema, and the bitmask is the low-level data 
    structure used to enable/disable tokens.
    """
    compiler = self._crane_build_xgr_compiler()
    cg = compiler.compile_json_schema(json.dumps(schema.model_json_schema()), any_whitespace=True, strict_mode=True)
    matcher = xgr.GrammarMatcher(cg)
    bitmask = xgr.allocate_token_bitmask(batch_size=1, vocab_size=self.n_vocab())
    return matcher, bitmask

# %% ../nbs/00_core.ipynb 19
from llama_cpp._internals import LlamaSampler

# %% ../nbs/00_core.ipynb 20
# In llama-cpp-xgrammar-crane.py
@patch
def _make_xgr_sampler(self: Llama, matcher: xgr.GrammarMatcher, bitmask: torch.Tensor) -> LlamaSampler:
    """
    Creates a custom LlamaSampler that integrates xgrammar constraints into the llama.cpp sampling pipeline.
    """
    from llama_cpp._internals import LlamaSampler

    # Determine the device to use for tensor operations
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def apply_func(cur_p):
        # 1) Get the candidate logits and their token IDs from llama.cpp's sampler.
        #    This is a PARTIAL list, not the full vocabulary.
        sz = cur_p.contents.size
        arr = np.ctypeslib.as_array(cur_p.contents.data, shape=(sz,))
        logits_np = arr["logit"]  # The logits for the candidates
        ids_np = arr["id"].astype(np.int32) # The token IDs for the candidates
        ids_t = torch.from_numpy(ids_np).to(device)

        # 2) If grammar has terminated, force EOS and exit early.
        if matcher.is_terminated():
            logits_np[...] = -np.inf
            eos_token_id = self.token_eos()
            # Find if EOS is in our candidate set and set its logit to 0.
            eos_rows = np.where(ids_np == eos_token_id)[0]
            if eos_rows.size > 0:
                logits_np[eos_rows[0]] = 0.0
            return

        # 3) Create a full-sized logits tensor initialized to negative infinity.
        #    This is the "Scatter" step.
        vocab_size = self.n_vocab()
        full_logits_t = torch.full((vocab_size,), -float('inf'), dtype=torch.float32, device=device)
        
        # 4) Place the candidate logits into the full tensor at their correct positions.
        full_logits_t[ids_t] = torch.from_numpy(logits_np).to(device)

        # 5) Compute the next-token bitmask from the grammar matcher.
        xgr.reset_token_bitmask(bitmask)
        matcher.fill_next_token_bitmask(bitmask, index=0)

        # 6) Apply the full-vocabulary bitmask to the full-vocabulary logits tensor.
        #    The bitmask is on the CPU, so we move it to the correct device.
        xgr.apply_token_bitmask_inplace(full_logits_t, bitmask.to(device), vocab_size=vocab_size)

        # 7) Gather the modified logits for the original candidates back into the numpy array
        #    that llama.cpp will read from. This is the "Gather" step.
        modified_logits = full_logits_t[ids_t].cpu().numpy()
        np.copyto(logits_np, modified_logits)
        
        # 8) Backstop: If all candidate logits became -inf (due to a grammar mismatch),
        #    force the EOS token to prevent the sampler from failing.
        if not np.isfinite(logits_np).any():
            logits_np[...] = -np.inf
            eos_token_id = self.token_eos()
            eos_rows = np.where(ids_np == eos_token_id)[0]
            if eos_rows.size > 0:
                logits_np[eos_rows[0]] = 0.0
            else:
                # If EOS wasn't even a candidate, just pick the first candidate to avoid a total crash.
                if len(logits_np) > 0:
                    logits_np[0] = 0.0

    sampler = LlamaSampler()
    sampler.add_custom(apply_func)
    sampler.add_greedy()
    return sampler

# %% ../nbs/00_core.ipynb 21
@patch
def _get_ctx(self: Llama):
    """
    Internal helper to safely access the underlying llama.cpp context
    """
    ctx = getattr(self, "_ctx", None)
    if ctx is None or not hasattr(ctx, "ctx"): raise TypeError("Expected a LlamaContext as self._ctx (with .ctx handle)")
    return ctx

# %% ../nbs/00_core.ipynb 22
def _find_subseq(a: List[int], sub: List[int]) -> int:
    """
    Helper function to find a sub-sequence of tokens.
    """
    if not sub: return -1
    L,M = len(a), len(sub)
    for i in range(max(0,L-M),L):
        if a[i:i+M] == sub: return i
    return -1

# %% ../nbs/00_core.ipynb 23
@patch
def _crane_generate_unconstrained(self: Llama, s1: str, max_toks: int, temperature: float, stop: List[str]) -> Generator[str, None, Tuple[List[int], str]]:
    """
    Generates text tokens by token in unconstrained mode.
    Yields: Detokenized text for each token
    Returns: A tuple of (all_generated_tokens, stop_reason)
    """
    s1_toks = self.tokenize(s1.encode("utf-8"), add_bos=False, special=True)
    stop_seq_toks = [self.tokenize(s.encode("utf-8"), add_bos=False, special=True) for s in stop if s]
    
    gen_toks: List[int] = []
    gen_text: str = ""

    for _ in range(max_toks): 
        tok = self.sample(temp=temperature)
        if tok == self.token_eos(): return gen_toks, "eos"

        gen_toks.append(tok)
        chunk = self.detokenize([tok], special=False).decode("utf-8", errors="ignore")
        gen_text += chunk; yield chunk

        for stop_seq in stop_seq_toks:
            seq_len = len(stop_seq)
            if len(gen_toks) >= seq_len and gen_toks[-seq_len:] == stop_seq: return gen_toks[:-seq_len], "stop"
        if len(gen_toks) >= len(s1_toks) and gen_toks[-len(s1_toks):] == s1_toks: return gen_toks, "s1"
        self.eval([tok])
    return gen_toks, "length"

# %% ../nbs/00_core.ipynb 24
@patch
def _crane_generate_constrained(self: Llama, schema: Type[BaseModel], s2_toks: List[int], max_toks:int, prefix_toks_after_s1: List[int] | None = None) -> Generator[str, None, List[int]]:
    """
        Generates text token by token in constrained mode.
        Yields: Detokenized text for each token.
        Returns: The list of tokens generated in this phase
    """
    matcher, bitmask = self._make_xgr_matcher(schema)
    
    sampler = self._make_xgr_sampler(matcher, bitmask)
    ctx = self._get_ctx()

    if prefix_toks_after_s1:
        for t in prefix_toks_after_s1: matcher.accept_token(t)
    
    generated_toks_in_phase: List[int] = []

    try:
        for _ in range(max_toks):
            tok = sampler.sample(ctx)
            if tok == self.token_eos() or matcher.is_terminated(): break
            generated_toks_in_phase.append(tok)

            sampler.accept(tok)
            matcher.accept_token(tok) # advance grammar for next step
            self.eval([tok]) # advance model KV    

            yield self.detokenize([tok], special=False).decode("utf-8", errors="ignore")

            if len(generated_toks_in_phase) >= len(s2_toks) and generated_toks_in_phase[-len(s2_toks):] == s2_toks: break
    finally:
        matcher.reset()
        sampler.reset()

    return generated_toks_in_phase
    

# %% ../nbs/00_core.ipynb 25
from llama_cpp.llama_chat_format import Jinja2ChatFormatter

# %% ../nbs/00_core.ipynb 27
@patch
def create_crane_chat_completion(self: Llama, messages: List[Dict[str,str]], schema=Type[BaseModel],
                                 s1:str = "<<JSON>>", s2:str="</JSON>>", temperature:float=0.0,
                                 max_tokens_unconstrained:int=3072,max_tokens_constrained:int=4096, 
                                 stop: List[str] = []) -> CreateCRANEChatCompletionResponse: 
        """
        CRANE: Constrained + Unconstrained switching for structured output.
        
        Generates text until `s1` delimiter (unconstrained), then switches to
        constrained JSON generation matching `pydantic_model` schema until `s2`.
        """
        from llama_cpp.llama_types import ChatCompletionResponseChoice, ChatCompletionResponseMessage
        # reset model state
        self.reset()
        etok = self._model.token_get_text(self.token_eos())
        btok = self._model.token_get_text(self.token_bos())
        formatter = Jinja2ChatFormatter(template=self.metadata['tokenizer.chat_template'],
                                        eos_token=etok, bos_token=btok)

        resp = formatter(messages=messages)
        f_prmpt: str = resp.prompt
        
        prmpt_toks = self.tokenize(f_prmpt.encode("utf-8"), add_bos=True)
        self.eval(prmpt_toks) # evaluate initial prompt
        
        # tokenize delimiters and stops
        s1_toks = self.tokenize(s1.encode("utf-8"), add_bos=False, special=True)
        s2_toks = self.tokenize(s2.encode("utf-8"), add_bos=False, special=True)
        
        # Phase 1. Unconstrained generation until s1 or max_tokens
        unc_gen = self._crane_generate_unconstrained(
                s1=s1, max_toks=max_tokens_unconstrained, temperature=temperature,
                stop=stop)
        unc_text, unc_toks, stop_reason = "", [], "error"
        while True:
                try: chunk = next(unc_gen)
                except StopIteration as e:
                        if e.value is not None: unc_toks, stop_reason = e.value
                        break
                unc_text += chunk
        
        if stop_reason != 's1':
                if s1 in unc_text: 
                        self.eval(s1_toks)
                        stop_reason = "s1"
                elif stop_reason == "eos":
                        self.eval(s1_toks)
                        unc_text += s1
                        stop_reason = "s1"
                else:
                        raise AssertionError(f"s1 delimiter '{s1}' not found in unconstrained phase.\n{unc_text}\n{stop_reason}")

        s1_pos = _find_subseq(unc_toks, s1_toks)
        if s1_pos == -1: prefix_toks_after_s1 = s1_toks
        else: prefix_toks_after_s1 = unc_toks[s1_pos:]
        self.eval(s1_toks)
        
        # Phase 2. Constrained generation
        con_gen = self._crane_generate_constrained(schema=schema, s2_toks=s2_toks, 
                                                              max_toks=max_tokens_constrained, prefix_toks_after_s1=prefix_toks_after_s1)
        jtxt, con_toks = "", []
        while True:
                try: chunk = next(con_gen)
                except StopIteration as e:
                        if e.value is not None: con_toks = e.value
                        break
                jtxt +=chunk
                
        unc_text += jtxt
        unc_text += s2
        #assert unc_text.endswith(s2), f"s2 delimiter '{s2}' not found at end of constrained phase.\nJTXT: {jtxt}"

        try:
                pjson = schema.model_validate_json(jtxt)
        except Exception as e: raise ValueError(f"Failed to parse generated JSON: {jtxt}\nError:{e}")
        
        cc = ChatCompletionResponseChoice(index=0,
                                message=ChatCompletionResponseMessage(role="assistant", content=jtxt), finish_reason="stop")
        
        return CreateCRANEChatCompletionResponse(id="crane-"+str(id(self)), object="crane.chat.completion",
                                                 completion=cc, json=pjson) 
